import os
import streamlit as st
import openai
import requests
from dotenv import load_dotenv
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 游댳 Cargar variables de entorno
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
WEATHER_API_KEY = os.getenv("WEATHER_API_KEY")

# 游댳 Configuraci칩n de modelos
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-3.5-turbo"

# 游댳 Cargar embeddings
embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

# 游댳 Directorio de datos (solo landmarks)
DATA_DIR = "data/landmark"

# 游댳 Funci칩n para cargar archivos de texto
def load_text_files(directory):
    texts = []
    for filename in os.listdir(directory):
        with open(os.path.join(directory, filename), "r", encoding="utf-8") as file:
            texts.append(file.read())
    return texts

# 游댳 Cargar solo data de landmarks
documents = load_text_files(DATA_DIR)

# 游댳 Dividir en chunks m치s peque침os para optimizar embeddings
text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=20)
flattened_docs = [chunk for doc in documents for chunk in text_splitter.split_text(doc)]

# 游댳 Evitar Rate Limit: procesar embeddings en lotes
BATCH_SIZE = 20
vector_store = FAISS.from_texts(flattened_docs[:BATCH_SIZE], embeddings)
retriever = vector_store.as_retriever()

# 游댳 Prompt actualizado con `query`
prompt_template = PromptTemplate(
    template="""
    You are an expert travel assistant for Puerto Rico.
    Generate an itinerary based on the following query:

    Query: {query}

    Example:
    User: "I have 3 days, I like nature and culture."
    Assistant: "Day 1: Visit El Yunque Rainforest..."
    
    Now generate the best itinerary based on the given information.
    """,
    input_variables=["query"]
)

# 游댳 Ajuste de RetrievalQA con `query` corregido
q
